{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovtRPGnBClBp"
      },
      "source": [
        "# RAG Implementation For Wikipedia Search Assistant\n",
        "\n",
        "## References:\n",
        "\n",
        "[Wikipedia]('https://en.wikipedia.org/wiki/')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX6_EK-OC8Hj"
      },
      "source": [
        "## Description\n",
        "The goal of this experiment is to implement a Retrieval-Augmented Generation (RAG) pipeline to answer user queries by combining document retrieval from Wikipedia with a language model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogBqRbYYFi5Q"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ9MCP86FwVv",
        "outputId": "5ea14aa9-2d92-4344-a282-138f84bbdd05"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"YOUR-PATH\""
      ],
      "metadata": {
        "id": "qE6VeiwaYKXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i4rA3qWClBt",
        "collapsed": true
      },
      "source": [
        "# Install necessary libraries\n",
        "%%capture\n",
        "!pip install langchain langchain-community wikipedia wikipedia-api faiss-cpu chromadb tiktoken sentence-transformers openai\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries. These libraries will be used to build the RAG pipeline\n",
        "from langchain.document_loaders import WikipediaLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import time\n",
        "from openai import OpenAIError  # Generic OpenAI exception class"
      ],
      "metadata": {
        "id": "D5QzCSiw7HX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up OpenAI API Key\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = 'YOUR-OPENAI-API-KEY'"
      ],
      "metadata": {
        "id": "Tc0keBgjYWGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Wikipedia Articles\n",
        "# Define a function to fetch Wikipedia articles and convert them into documents\n",
        "def load_wikipedia_articles(queries):\n",
        "    documents = []\n",
        "    for query in queries:\n",
        "        try:\n",
        "            loader = WikipediaLoader(query=query)\n",
        "            docs = loader.load()\n",
        "            documents.extend(docs)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {query}: {e}\")\n",
        "    return documents"
      ],
      "metadata": {
        "id": "4TKzk11-7Hep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Load Wikipedia articles based on user-defined queries\n",
        "queries = [\"Artificial Intelligence\", \"Machine Learning\", \"Natural Language Processing\"]\n",
        "documents = load_wikipedia_articles(queries)\n",
        "print(f\"Loaded {len(documents)} documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuCGyGt47HhQ",
        "outputId": "450c8f02-a985-4cfa-cfde-a822cd4b9d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 75 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the embedding model\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # Local embedding model\n",
        "\n",
        "# Use HuggingFaceEmbeddings wrapper for compatibility with LangChain\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
      ],
      "metadata": {
        "id": "3fvSXu-bNDcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Chroma Vector Store\n",
        "# Store the embeddings in a Chroma database\n",
        "vectorstore = Chroma.from_documents(documents, embeddings)\n",
        "print(\"Chroma vector store created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nCFc_KyO75y",
        "outputId": "ddf3f427-0bdd-4ddd-ebe9-22964590f225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chroma vector store created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Up RAG Chain\n",
        "# Combine retriever with an LLM (GPT-4 or GPT-3.5)\n",
        "retriever = vectorstore.as_retriever()\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=ChatOpenAI(model=\"gpt-4\"),\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1RnA1rFPjcu",
        "outputId": "5a08c6e1-2ad9-4fe2-9455-d4847e26c758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-b29eb04d0d93>:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm=ChatOpenAI(model=\"gpt-4\"),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_questions_in_batches(queries, batch_size=3, delay=10):\n",
        "    results = []\n",
        "    for i in range(0, len(queries), batch_size):\n",
        "        batch = queries[i:i+batch_size]\n",
        "        for query in batch:\n",
        "            try:\n",
        "                print(f\"Processing query: {query}\")\n",
        "                result = qa_chain({\"query\": query})\n",
        "                results.append({\n",
        "                    \"query\": query,\n",
        "                    \"answer\": result[\"result\"],\n",
        "                    \"sources\": [doc.metadata.get(\"source\", \"Unknown Source\") for doc in result[\"source_documents\"]]\n",
        "                })\n",
        "                time.sleep(1)  # Small delay between individual queries\n",
        "            except OpenAIError as e:\n",
        "                print(f\"OpenAI API error: {e}. Retrying in {delay} seconds...\")\n",
        "                time.sleep(delay)\n",
        "                continue\n",
        "    return results"
      ],
      "metadata": {
        "id": "q1jJc41SPjfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Query\n",
        "user_query = \"What is the impact of Artificial Intelligence on society?\"\n",
        "ask_questions_in_batches([user_query])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvXVnbCyP8AE",
        "outputId": "c9ff6b4c-795e-4c6f-b76a-82ce35036c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing query: What is the impact of Artificial Intelligence on society?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'query': 'What is the impact of Artificial Intelligence on society?',\n",
              "  'answer': 'Artificial Intelligence (AI) has a significant impact on society in various ways. Here are a few examples:\\n\\n1. Efficiency and Productivity: AI can automate repetitive tasks, freeing up time for individuals to focus on more complex tasks that require critical thinking and personal touch.\\n\\n2. Decision-making: AI can analyze vast amounts of data to identify patterns and trends that humans might overlook, aiding in decision-making processes in fields such as finance or healthcare.\\n\\n3. Medical Diagnosis and Research: AI can assist doctors in diagnosing diseases or suggest treatment plans. It can also speed up the process of drug discovery and medical research.\\n\\n4. Agriculture: AI can help farmers monitor crop and soil health, predict weather conditions, and optimize resource usage, leading to increased crop yields and sustainability.\\n\\n5. Job Market Changes: While AI can lead to job displacement in certain sectors due to automation, it can also create new jobs that require skills in AI and machine learning.\\n\\n6. Regulation and Ethics: As AI becomes more pervasive, there are growing concerns about data privacy, job displacement, and ethical issues such as bias in AI algorithms. This has led to increased calls for regulation of AI.\\n\\n7. Existential Risks: There are debates about the potential risks of AI surpassing human intelligence, leading to scenarios where AI could become uncontrollable, posing existential risks to humanity.\\n\\n8. Accessibility: AI can make technology more accessible for people with disabilities. For example, voice recognition can help those with mobility issues to control devices using their voice.\\n\\nHence, while AI has the potential to drive innovation and improve efficiency in many sectors, it also presents challenges that society needs to address.',\n",
              "  'sources': ['https://en.wikipedia.org/wiki/Progress_in_artificial_intelligence',\n",
              "   'https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence',\n",
              "   'https://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence',\n",
              "   'https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence']}]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    }
  ]
}